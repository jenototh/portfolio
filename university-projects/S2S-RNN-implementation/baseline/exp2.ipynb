{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Select An Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "architecture = \"best\" # choose this for the overall best architecture\n",
        "# architecture = \"task\" # choose this for the task specific best architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prepare Data\n",
        "Import libraries, download text data from github, set seeds for reproducability, create vocabulary, dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjdomJvuMvel"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.utils as torch_utils\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def download_file(file_url):\n",
        "    response = requests.get(file_url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        print(f\"Failed to fetch the file. Status code: {response.status_code}\")\n",
        "        return None\n",
        "    \n",
        "# URLs of the files you want to download\n",
        "file_urls = [\n",
        "    'https://raw.githubusercontent.com/brendenlake/SCAN/master/length_split/tasks_train_length.txt',\n",
        "    'https://raw.githubusercontent.com/brendenlake/SCAN/master/length_split/tasks_test_length.txt'  # Replace with the actual URL of the second file\n",
        "]\n",
        "\n",
        "# Use ThreadPoolExecutor to download files concurrently\n",
        "with ThreadPoolExecutor(max_workers=len(file_urls)) as executor:\n",
        "    results = list(executor.map(download_file, file_urls))\n",
        "\n",
        "# Check if all downloads were successful\n",
        "if all(result is not None for result in results):\n",
        "    # Assuming each line in the text file is a sentence\n",
        "    sentences_list = [result.split('\\n') for result in results]\n",
        "\n",
        "    # Create DataFrames for each file\n",
        "    dfs = [pd.DataFrame({'Sentences': sentences}) for sentences in sentences_list]\n",
        "\n",
        "    # Create DataFrames for each file\n",
        "    train_df_exp2 = pd.DataFrame({'Sentences': sentences_list[0]})\n",
        "    eval_df_exp2 = pd.DataFrame({'Sentences': sentences_list[1]})\n",
        "else:\n",
        "    print(\"One or more downloads failed.\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyHBIxL7MpPQ",
        "outputId": "4ce6adbd-a48a-4d21-c19e-eec1db4e5366"
      },
      "outputs": [],
      "source": [
        "SEED = 3\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eu5e_1PHK107"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SentenceDataset(Dataset):\n",
        "   def __init__(self, sentences):\n",
        "       self.sentences = sentences\n",
        "\n",
        "   def __len__(self):\n",
        "       return len(self.sentences)\n",
        "\n",
        "   def __getitem__(self, idx):\n",
        "       sentence = self.sentences[idx]\n",
        "       in_sentence = sentence.split('IN:')[1].split('OUT:')[0].strip()\n",
        "       out_sentence = sentence.split('OUT:')[1].strip()\n",
        "       return [in_sentence, out_sentence]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c7D_Q8oN8xZ"
      },
      "outputs": [],
      "source": [
        "PAD_token = 0\n",
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
        "        self.n_words = 3  # Count PAD SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwM7rCbxRy1Z"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(vocab, sentence):\n",
        "    return [vocab.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_set_length = SentenceDataset(train_df_exp2['Sentences'])\n",
        "eval_set_length = SentenceDataset(eval_df_exp2['Sentences'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPxtmq1JSD-x"
      },
      "outputs": [],
      "source": [
        "input_vocab = Vocab('IN')\n",
        "output_vocab = Vocab('OUT')\n",
        "\n",
        "for line in train_set_length:\n",
        "    input_vocab.addSentence(line[0])\n",
        "    output_vocab.addSentence(line[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhQ9uZkZxOyC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset, in_voacab, out_vocab):\n",
        "        self.dataset = dataset\n",
        "        self.in_vocab = in_voacab\n",
        "        self.out_vocab = out_vocab\n",
        "        input_sentences = []\n",
        "        target_sentences = []\n",
        "        for line in self.dataset:\n",
        "            input_sentences.append(line[0])\n",
        "            target_sentences.append(line[1])\n",
        "        self.input_sentences = input_sentences\n",
        "        self.target_sentences = target_sentences\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_sentence = self.input_sentences[idx]\n",
        "        target_sentence = self.target_sentences[idx]\n",
        "\n",
        "        # Convert words to indices using word2index dictionary\n",
        "        input_indices = indexesFromSentence(self.in_vocab, input_sentence)\n",
        "        input_indices.append(EOS_token)\n",
        "        target_indices = indexesFromSentence(self.out_vocab, target_sentence)\n",
        "        target_indices.append(EOS_token)\n",
        "\n",
        "        return torch.LongTensor(input_indices).to(device), torch.LongTensor(target_indices).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGf1joDxvjvd"
      },
      "source": [
        "# Model Definitions\n",
        "Includes both overall best and task best architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUL2TZUIvnjH"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.5):\n",
        "        super(EncoderLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.LSTM = nn.LSTM(hidden_size, hidden_size, num_layers=2, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        output, hidden = self.LSTM(embedded)\n",
        "        return output, hidden\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.5):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3Ew4tSH5DEL"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.5):\n",
        "        super(DecoderLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.LSTM = nn.LSTM(hidden_size, hidden_size, num_layers=2, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, epoch=0, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        if target_tensor is not None:\n",
        "            target_len = target_tensor.size(1)\n",
        "        else:\n",
        "            target_len = 48\n",
        "\n",
        "        for i in range(target_len):\n",
        "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            if target_tensor is not None and epoch < 50000:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
        "\n",
        "    def forward_step(self, input, hidden):\n",
        "        output = self.dropout(self.embedding(input))\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.LSTM(output, hidden)\n",
        "        output = self.out(output)\n",
        "        return output, hidden\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights\n",
        "\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.5):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, epoch=0, target_tensor=None, oracle=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        if target_tensor is not None:\n",
        "            target_len = target_tensor.size(1)\n",
        "        else:\n",
        "            target_len = 48\n",
        "\n",
        "        if oracle is not None:\n",
        "            min_len = oracle\n",
        "\n",
        "        for i in range(target_len):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None and epoch < 50000:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                # Code also includes lenght oracle as debugging option\n",
        "                _, topi = decoder_output.topk(2)\n",
        "                top1 = topi[0,0,0].item()\n",
        "                top2 = topi[0,0,1].item()\n",
        "\n",
        "                if oracle is not None and i < min_len and top1 == EOS_token:\n",
        "                    decoder_input = torch.tensor([top2]).to(device).unsqueeze(1).detach()  # detach from history as input\n",
        "                else:\n",
        "                    decoder_input = torch.tensor([top1]).to(device).unsqueeze(1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7YyV0EdOnP6"
      },
      "source": [
        "# Train encoder decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMZRvjFAZIoK"
      },
      "outputs": [],
      "source": [
        "import torch.nn.utils as torch_utils\n",
        "\n",
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
        "          decoder_optimizer, criterion, epoch, max_norm=5.0):\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    input_tensor, target_tensor = next(iter(dataloader))\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "    decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, epoch,target_tensor)\n",
        "\n",
        "    loss = criterion(\n",
        "        decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "        target_tensor.view(-1)\n",
        "    )\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    # Gradient clipping for both encoder and decoder\n",
        "    torch_utils.clip_grad_norm_(encoder.parameters(), max_norm)\n",
        "    torch_utils.clip_grad_norm_(decoder.parameters(), max_norm)\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cENky90-5V_F"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAq3Oe6p5kII"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B69llEg65akQ"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
        "               print_every=100, plot_every=100):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, epoch)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Choose Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "\n",
        "train_dataset = CustomDataset(train_set_length, input_vocab, output_vocab)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "if architecture == \"best\":\n",
        "    hidden_size = 200\n",
        "    encoder = EncoderLSTM(input_vocab.n_words, hidden_size).to(device)\n",
        "    decoder = DecoderLSTM(hidden_size, output_vocab.n_words).to(device)\n",
        "elif architecture == \"task\":\n",
        "    hidden_size = 50\n",
        "    encoder = EncoderRNN(input_vocab.n_words, hidden_size).to(device)\n",
        "    decoder = AttnDecoderRNN(hidden_size, output_vocab.n_words).to(device)\n",
        "\n",
        "train(train_dataloader, encoder, decoder, 100000, print_every=100, plot_every=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# torch.save(encoder, f\"rnnencoder{SEED}\")\n",
        "# torch.save(decoder, f\"rnndecoder{SEED}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlIlFK1JxLEG",
        "outputId": "310deb30-da0d-432f-a4d2-72986bfbfd6f"
      },
      "outputs": [],
      "source": [
        "cmd_lengths_compact = [4, 6, 7, 8, 9]\n",
        "act_lengths_compact = [24, 25, 26, 27, 28, 30, 32, 33, 36, 40, 48]\n",
        "\n",
        "def evaluate(encoder, decoder, test_sentences, input_lang, output_lang):\n",
        "    success = 0\n",
        "    success_partial = 0\n",
        "\n",
        "    MAX_LENGTH = 49\n",
        "    cmd_lengths = np.zeros(10)\n",
        "    act_lengths = np.zeros(MAX_LENGTH)\n",
        "    cmd_acc = np.zeros(10)\n",
        "    act_acc = np.zeros(MAX_LENGTH)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(test_sentences)):\n",
        "            print(i)\n",
        "            input_sentence = test_sentences[i][0]\n",
        "            input_tensor = tensorFromSentence(input_lang, input_sentence)\n",
        "\n",
        "            cmd_len = len(input_sentence.split())\n",
        "            act_len = len(test_sentences[i][1].split())\n",
        "\n",
        "            encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "\n",
        "            decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden, epoch=1)\n",
        "            _, topi = decoder_outputs.topk(1)\n",
        "            decoded_ids = topi.squeeze()\n",
        "\n",
        "            # -------------------- LENGHT ORACLE DEBUGGING --------------------------\n",
        "            # min_len = act_len\n",
        "            # decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, oracle=act_len)\n",
        "            # _, topi = decoder_outputs.topk(2)\n",
        "            # decoded_ids = topi.squeeze()\n",
        "            # decoded_ids = torch.tensor([top2 if idx < min_len and top1 == EOS_token else top1 for idx, [top1, top2] in enumerate(decoded_ids.to('cpu').numpy())])\n",
        "\n",
        "            # try:\n",
        "            #     index = (decoded_ids == EOS_token).nonzero()[0].item()\n",
        "            #     decoded_ids = decoded_ids[:index + 1]\n",
        "            # except:\n",
        "            #     # pass\n",
        "            #     print(decoded_ids, i)\n",
        "\n",
        "\n",
        "            decoded_words = []\n",
        "            for idx in decoded_ids:\n",
        "                if idx.item() == EOS_token:\n",
        "                    break\n",
        "                decoded_words.append(output_lang.index2word[idx.item()])\n",
        "\n",
        "\n",
        "            cmd_lengths[cmd_len] += 1\n",
        "            act_lengths[act_len] += 1\n",
        "\n",
        "            if (decoded_words == test_sentences[i][1].split()):\n",
        "                success = success + 1\n",
        "                cmd_acc[cmd_len] += 1\n",
        "                act_acc[act_len] += 1\n",
        "                print(\"succes \", success, \"out of\", i+1)\n",
        "            #check partial match\n",
        "            if (test_sentences[i][1] in \" \".join(decoded_words)):\n",
        "                success_partial = success_partial + 1\n",
        "                print(\"partial succes \", success_partial, \"out of\", i+1)\n",
        "\n",
        "    cmd_lengths, act_lengths = cmd_lengths[cmd_lengths_compact], act_lengths[act_lengths_compact]\n",
        "    cmd_acc, act_acc = cmd_acc[cmd_lengths_compact], act_acc[act_lengths_compact]\n",
        "\n",
        "    cmd_acc = cmd_acc/cmd_lengths * 100\n",
        "    act_acc = act_acc/act_lengths * 100\n",
        "\n",
        "    print(f\"Exact match accuracy: {success / len(test_sentences) * 100:.2f}%\")\n",
        "    print(f\"Partial match accuracy: {success_partial / len(test_sentences) * 100:.2f}%\")\n",
        "    return (success / len(test_sentences) * 100), cmd_acc, act_acc\n",
        "\n",
        "eval = [eval_set_length[i] for i in range(3920)]\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "avg, cmd, act = evaluate(encoder, decoder, eval, input_vocab, output_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAybX7FozfQx",
        "outputId": "f0bb7711-f83b-4bf5-ca98-a3d669cd685b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "ax1.bar([str(len) for len in cmd_lengths_compact], cmd, color='lightsteelblue')\n",
        "ax2.bar([str(len) for len in act_lengths_compact], act, color='lightsteelblue')\n",
        "\n",
        "ax1.set_ylim(top = 100)\n",
        "ax2.set_ylim(top = 100)\n",
        "\n",
        "ax1.set_ylabel(\"Accuracy on new commands (%)\")\n",
        "ax2.set_ylabel(\"Accuracy on new commands (%)\")\n",
        "\n",
        "ax1.set_xlabel(\"Ground-truth action sequence length\")\n",
        "ax2.set_xlabel(\"Command length\")\n",
        "\n",
        "print(\"cmd lenghts in order: \", cmd_lengths_compact)\n",
        "print(\"cmd lenghts in order: \", act_lengths_compact)\n",
        "print()\n",
        "print(\"exact match accuracy:               \", avg)\n",
        "print(\"accuracy by command length:         \", cmd)\n",
        "print(\"accuracy by action sequence lenght: \", act)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

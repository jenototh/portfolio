{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
    "        self.n_words = 3  # Count PAD SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData():\n",
    "    print(\"reading train lines...\")\n",
    "    lines = open(\"tasks_train_addprim_jump.txt\").read().strip().split(\"\\n\")\n",
    "    train_pairs = [[re.sub(\"IN: \", '', seq.strip()) for seq in seq.split(\"OUT: \")] for seq in lines]\n",
    "\n",
    "    print(\"reading test lines...\")\n",
    "    lines = open(\"tasks_test_addprim_jump.txt\").read().strip().split(\"\\n\")\n",
    "    test_pairs = [[re.sub(\"IN: \", '', seq.strip()) for seq in seq.split(\"OUT: \")] for seq in lines]\n",
    "    \n",
    "\n",
    "    print(\"Building dictionaries...\")\n",
    "    input_lang = Vocab('in')\n",
    "    output_lang = Vocab('out')\n",
    "\n",
    "    print(\"Data loading complete\")\n",
    "    \n",
    "    return input_lang, output_lang, train_pairs, test_pairs\n",
    "\n",
    "input_lang, output_lang, train_pairs, test_pairs = prepareData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(vocab, sentence):\n",
    "    return [vocab.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "input_vocab = Vocab('IN')\n",
    "output_vocab = Vocab('OUT')\n",
    "for line in train_pairs:\n",
    "    input_vocab.addSentence(line[0])\n",
    "    output_vocab.addSentence(line[1])\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, in_voacab, out_vocab):\n",
    "        self.dataset = dataset\n",
    "        self.in_vocab = in_voacab\n",
    "        self.out_vocab = out_vocab\n",
    "        input_sentences = []\n",
    "        target_sentences = []\n",
    "        for line in self.dataset:\n",
    "            input_sentences.append(line[0])\n",
    "            target_sentences.append(line[1])\n",
    "        self.input_sentences = input_sentences\n",
    "        self.target_sentences = target_sentences\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_sentence = self.input_sentences[idx]\n",
    "        target_sentence = self.target_sentences[idx]\n",
    "\n",
    "        # Convert words to indices using word2index dictionary\n",
    "        input_indices = indexesFromSentence(self.in_vocab, input_sentence)\n",
    "        input_indices.append(EOS_token)\n",
    "        target_indices = indexesFromSentence(self.out_vocab, target_sentence)\n",
    "        target_indices.append(EOS_token)\n",
    "\n",
    "        return torch.LongTensor(input_indices).to(device), torch.LongTensor(target_indices).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.LSTM = nn.LSTM(hidden_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        output, hidden = self.LSTM(embedded)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: [batch size, hidden size]\n",
    "        # encoder_outputs: [batch size, sequence length, hidden size]\n",
    "\n",
    "        # Repeat hidden state for each time step in the sequence\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, encoder_outputs.size(1), 1)\n",
    "\n",
    "        # Calculate energy\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        energy = energy.transpose(1, 2)  # [batch size, hidden size, sequence length]\n",
    "\n",
    "        # Calculate attention\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # [batch size, 1, hidden size]\n",
    "        attention = torch.bmm(v, energy).squeeze(1)  # [batch size, sequence length]\n",
    "        return F.softmax(attention, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.LSTM = nn.LSTM(hidden_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        #print(\"decoder input\", decoder_input)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        if target_tensor is not None:\n",
    "            target_len = target_tensor.size(1)\n",
    "        else:\n",
    "            target_len = 60\n",
    "        #print(\"target len:\", trg_len)\n",
    "        #print(\"decoder hidden\", decoder_hidden)\n",
    "\n",
    "        for i in range(target_len):\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            #print(\"decoder single output\", decoder_output)\n",
    "            #geci\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                #print(\"target tensor\", target_tensor)\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "                #print(\"Decoder input case 1 (target)\", decoder_input)\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "                #print(\"Decoder input case 2 (topi)\", decoder_input)\n",
    "\n",
    "        #print(\"decoder outputs\", decoder_outputs)\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.dropout(self.embedding(input))\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.LSTM(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "    \n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size + hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        # Use the actual length of the target tensor if provided\n",
    "        max_target_length = target_tensor.size(1) if target_tensor is not None else MAX_LENGTH\n",
    "\n",
    "        for i in range(max_target_length):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)  # Teacher forcing\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        query = hidden[0].permute(1, 0, 2)  # Use the hidden state (h_n) for attention\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_lstm = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.lstm(input_lstm, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils as torch_utils\n",
    "\n",
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion, max_norm=5.0):\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    input_tensor, target_tensor = next(iter(dataloader))\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "    decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "    #print(\"Decoder that get passed =\", decoder_outputs.view(-1, decoder_outputs.size(-1)))\n",
    "    #print(\"target_tensor that get passed =\", target_tensor.view(-1))\n",
    "    loss = criterion(\n",
    "        decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "        target_tensor.view(-1)\n",
    "    )\n",
    "\n",
    "    #print(\"decoder_outputs\", decoder_outputs)\n",
    "    #print(\"target_tensor\", target_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient clipping for both encoder and decoder\n",
    "    torch_utils.clip_grad_norm_(encoder.parameters(), max_norm)\n",
    "    torch_utils.clip_grad_norm_(decoder.parameters(), max_norm)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    return total_loss\n",
    "    #return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, test_sentences, input_lang, output_lang):\n",
    "    success = 0\n",
    "    success_partial = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_sentences)):\n",
    "            #print(i)\n",
    "            input_sentence = test_sentences[i][0]\n",
    "            input_tensor = tensorFromSentence(input_lang, input_sentence)\n",
    "\n",
    "            encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "            decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "            _, topi = decoder_outputs.topk(1)\n",
    "            decoded_ids = topi.squeeze()\n",
    "\n",
    "            decoded_words = []\n",
    "            for idx in decoded_ids:\n",
    "                if idx.item() == EOS_token:\n",
    "                    break\n",
    "                decoded_words.append(output_lang.index2word[idx.item()])\n",
    "            #check exact match between decoded_words and test_dataset[i][1]\n",
    "            #print(\"Expected output: \" + test_sentences[i][1])\n",
    "            #print(\"Predicted output: \" + \" \".join(decoded_words))\n",
    "\n",
    "            #check exact match\n",
    "            if (decoded_words == test_sentences[i][1].split()):\n",
    "                success += 1\n",
    "                #print(\"succes \", success, \"out of\", i+1)\n",
    "                #print(\"SUCCESS!\")\n",
    "                #print(\"TARGET:\")\n",
    "                #print(test_sentences[i][1])\n",
    "                #print(\"PREDICTED:\")\n",
    "                #print(decoded_words)\n",
    "            #check partial match\n",
    "            if (test_sentences[i][1] in \" \".join(decoded_words)):\n",
    "                success_partial = success_partial + 1\n",
    "                #print(\"partial succes \", success_partial, \"out of\", i+1)\n",
    "                #print(\"PARTIAL SUCCESS!\")\n",
    "                #print(\"TARGET:\")\n",
    "                #print(test_sentences[i][1])\n",
    "                #print(\"PREDICTED:\")\n",
    "                #print(\" \".join(decoded_words))\n",
    "\n",
    "\n",
    "    print(f\"Exact match accuracy: {success / len(test_sentences) * 100:.2f}%\")\n",
    "    print(f\"Partial match accuracy: {success_partial / len(test_sentences) * 100:.2f}%\")\n",
    "    return (success / len(test_sentences) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE\n",
    "\n",
    "def prepareData():\n",
    "    print(\"reading train lines...\")\n",
    "    lines = open(\"tasks_train_addprim_jump.txt\").read().strip().split(\"\\n\")\n",
    "    train_pairs = [[re.sub(\"IN: \", '', seq.strip()) for seq in seq.split(\"OUT: \")] for seq in lines]\n",
    "\n",
    "    print(\"reading test lines...\")\n",
    "    lines = open(\"tasks_test_addprim_jump.txt\").read().strip().split(\"\\n\")\n",
    "    test_pairs = [[re.sub(\"IN: \", '', seq.strip()) for seq in seq.split(\"OUT: \")] for seq in lines]\n",
    "    \n",
    "\n",
    "    print(\"Building dictionaries...\")\n",
    "    input_lang = Vocab('in')\n",
    "    output_lang = Vocab('out')\n",
    "\n",
    "    print(\"Data loading complete\")\n",
    "    \n",
    "    return input_lang, output_lang, train_pairs, test_pairs\n",
    "\n",
    "input_lang, output_lang, train_pairs, test_pairs = prepareData()\n",
    "\n",
    "input_vocab = Vocab('IN')\n",
    "output_vocab = Vocab('OUT')\n",
    "for line in train_pairs:\n",
    "    input_vocab.addSentence(line[0])\n",
    "    output_vocab.addSentence(line[1])\n",
    "\n",
    "hidden_size = 100\n",
    "batch_size = 1\n",
    "\n",
    "train_dataset = CustomDataset(train_pairs, input_vocab, output_vocab)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "encoder = EncoderLSTM(input_vocab.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_vocab.n_words).to(device)\n",
    "#decoder_no_attn = DecoderLSTM(hidden_size, output_vocab.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 100000, print_every=1000, plot_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(encoder, \"encoder_LSTM_exp3_wPad_wAttn\")\n",
    "#torch.save(decoder, \"decoder_LSTM_exp3_wPad_wAttn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "eval = [test_pairs[i] for i in range(len(test_pairs))]\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "result_baseline = evaluate(encoder, decoder, eval, input_vocab, output_vocab)\n",
    "results.append(result_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP FOR EACH EXPERIMENT!\n",
    "#1\n",
    "def prepareData():\n",
    "    print(\"reading train lines...\")\n",
    "    lines = open(\"ATNLProject-main/Data/add_prim_split/with_additional_examples/tasks_train_addprim_complex_jump_num1_rep1.txt\").read().strip().split(\"\\n\")\n",
    "    train_pairs = [[re.sub(\"IN: \", '', seq.strip()) for seq in seq.split(\"OUT: \")] for seq in lines]\n",
    "\n",
    "    print(\"reading test lines...\")\n",
    "    lines = open(\"ATNLProject-main/Data/add_prim_split/with_additional_examples/tasks_test_addprim_complex_jump_num1_rep1.txt\").read().strip().split(\"\\n\")\n",
    "    test_pairs = [[re.sub(\"IN: \", '', seq.strip()) for seq in seq.split(\"OUT: \")] for seq in lines]\n",
    "    \n",
    "\n",
    "    print(\"Building dictionaries...\")\n",
    "    input_lang = Vocab('in')\n",
    "    output_lang = Vocab('out')\n",
    "\n",
    "    print(\"Data loading complete\")\n",
    "    \n",
    "    return input_lang, output_lang, train_pairs, test_pairs\n",
    "\n",
    "input_lang, output_lang, train_pairs, test_pairs = prepareData()\n",
    "\n",
    "input_vocab = Vocab('IN')\n",
    "output_vocab = Vocab('OUT')\n",
    "for line in train_pairs:\n",
    "    input_vocab.addSentence(line[0])\n",
    "    output_vocab.addSentence(line[1])\n",
    "\n",
    "\n",
    "hidden_size = 100\n",
    "batch_size = 1\n",
    "\n",
    "train_dataset = CustomDataset(train_pairs, input_vocab, output_vocab)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "encoder = EncoderLSTM(input_vocab.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_vocab.n_words).to(device)\n",
    "#decoder_no_attn = DecoderLSTM(hidden_size, output_vocab.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 100000, print_every=10000, plot_every=5000)\n",
    "\n",
    "torch.save(encoder, \"encoder_LSTM_exp3_wPad_wAttn_1\")\n",
    "torch.save(decoder, \"decoder_LSTM_exp3_wPad_wAttn_1\")\n",
    "\n",
    "\n",
    "eval = [test_pairs[i] for i in range(len(test_pairs))]\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "result_1 = evaluate(encoder, decoder, eval, input_vocab, output_vocab)\n",
    "results.append(result_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP FOR EACH EXPERIMENT!\n",
    "#2\n",
    "def prepareData():\n",
    "    print(\"reading train lines...\")\n",
    "    lines = open(\"ATNLProject-main/Data/add_prim_split/with_additional_examples/tasks_train_addprim_complex_jump_num2_rep1.txt\").read().strip().split(\"\\n\")\n",
    "    train_pairs = [[re.sub(\"IN: \", '', seq.strip()) for seq in seq.split(\"OUT: \")] for seq in lines]\n",
    "\n",
    "    print(\"reading test lines...\")\n",
    "    lines = open(\"ATNLProject-main/Data/add_prim_split/with_additional_examples/tasks_test_addprim_complex_jump_num2_rep1.txt\").read().strip().split(\"\\n\")\n",
    "    test_pairs = [[re.sub(\"IN: \", '', seq.strip()) for seq in seq.split(\"OUT: \")] for seq in lines]\n",
    "    \n",
    "\n",
    "    print(\"Building dictionaries...\")\n",
    "    input_lang = Vocab('in')\n",
    "    output_lang = Vocab('out')\n",
    "\n",
    "    print(\"Data loading complete\")\n",
    "    \n",
    "    return input_lang, output_lang, train_pairs, test_pairs\n",
    "\n",
    "input_lang, output_lang, train_pairs, test_pairs = prepareData()\n",
    "\n",
    "input_vocab = Vocab('IN')\n",
    "output_vocab = Vocab('OUT')\n",
    "for line in train_pairs:\n",
    "    input_vocab.addSentence(line[0])\n",
    "    output_vocab.addSentence(line[1])\n",
    "\n",
    "\n",
    "hidden_size = 100\n",
    "batch_size = 1\n",
    "\n",
    "train_dataset = CustomDataset(train_pairs, input_vocab, output_vocab)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "encoder = EncoderLSTM(input_vocab.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_vocab.n_words).to(device)\n",
    "#decoder_no_attn = DecoderLSTM(hidden_size, output_vocab.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 100000, print_every=10000, plot_every=5000)\n",
    "\n",
    "torch.save(encoder, \"encoder_LSTM_exp3_wPad_wAttn_2\")\n",
    "torch.save(decoder, \"decoder_LSTM_exp3_wPad_wAttn_2\")\n",
    "\n",
    "eval = [test_pairs[i] for i in range(len(test_pairs))]\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "result_2 = evaluate(encoder, decoder, eval, input_vocab, output_vocab)\n",
    "results.append(result_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP FOR EACH EXPERIMENT!\n",
    "#4\n",
    "def prepareData():\n",
    "    print(\"reading train lines...\")\n",
    "    lines = open(\"ATNLProject-main/Data/add_prim_split/with_additional_examples/tasks_train_addprim_complex_jump_num4_rep1.txt\").read().strip().split(\"\\n\")\n",
    "    train_pairs = [[re.sub(\"IN: \", '', seq.strip()) for seq in seq.split(\"OUT: \")] for seq in lines]\n",
    "\n",
    "    print(\"reading test lines...\")\n",
    "    lines = open(\"ATNLProject-main/Data/add_prim_split/with_additional_examples/tasks_test_addprim_complex_jump_num4_rep1.txt\").read().strip().split(\"\\n\")\n",
    "    test_pairs = [[re.sub(\"IN: \", '', seq.strip()) for seq in seq.split(\"OUT: \")] for seq in lines]\n",
    "    \n",
    "\n",
    "    print(\"Building dictionaries...\")\n",
    "    input_lang = Vocab('in')\n",
    "    output_lang = Vocab('out')\n",
    "\n",
    "    print(\"Data loading complete\")\n",
    "    \n",
    "    return input_lang, output_lang, train_pairs, test_pairs\n",
    "\n",
    "input_lang, output_lang, train_pairs, test_pairs = prepareData()\n",
    "\n",
    "input_vocab = Vocab('IN')\n",
    "output_vocab = Vocab('OUT')\n",
    "for line in train_pairs:\n",
    "    input_vocab.addSentence(line[0])\n",
    "    output_vocab.addSentence(line[1])\n",
    "\n",
    "\n",
    "hidden_size = 100\n",
    "batch_size = 1\n",
    "\n",
    "train_dataset = CustomDataset(train_pairs, input_vocab, output_vocab)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "encoder = EncoderLSTM(input_vocab.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_vocab.n_words).to(device)\n",
    "#decoder_no_attn = DecoderLSTM(hidden_size, output_vocab.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 100000, print_every=10000, plot_every=5000)\n",
    "\n",
    "torch.save(encoder, \"encoder_LSTM_exp3_wPad_wAttn_4\")\n",
    "torch.save(decoder, \"decoder_LSTM_exp3_wPad_wAttn_4\")\n",
    "\n",
    "eval = [test_pairs[i] for i in range(len(test_pairs))]\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "result_4 = evaluate(encoder, decoder, eval, input_vocab, output_vocab)\n",
    "results.append(result_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP FOR EACH EXPERIMENT!\n",
    "#8\n",
    "def prepareData():\n",
    "    print(\"reading train lines...\")\n",
    "    lines = open(\"ATNLProject-main/Data/add_prim_split/with_additional_examples/tasks_train_addprim_complex_jump_num8_rep1.txt\").read().strip().split(\"\\n\")\n",
    "    train_pairs = [[re.sub(\"IN: \", '', seq.strip()) for seq in seq.split(\"OUT: \")] for seq in lines]\n",
    "\n",
    "    print(\"reading test lines...\")\n",
    "    lines = open(\"ATNLProject-main/Data/add_prim_split/with_additional_examples/tasks_test_addprim_complex_jump_num8_rep1.txt\").read().strip().split(\"\\n\")\n",
    "    test_pairs = [[re.sub(\"IN: \", '', seq.strip()) for seq in seq.split(\"OUT: \")] for seq in lines]\n",
    "    \n",
    "\n",
    "    print(\"Building dictionaries...\")\n",
    "    input_lang = Vocab('in')\n",
    "    output_lang = Vocab('out')\n",
    "\n",
    "    print(\"Data loading complete\")\n",
    "    \n",
    "    return input_lang, output_lang, train_pairs, test_pairs\n",
    "\n",
    "input_lang, output_lang, train_pairs, test_pairs = prepareData()\n",
    "\n",
    "input_vocab = Vocab('IN')\n",
    "output_vocab = Vocab('OUT')\n",
    "for line in train_pairs:\n",
    "    input_vocab.addSentence(line[0])\n",
    "    output_vocab.addSentence(line[1])\n",
    "\n",
    "\n",
    "hidden_size = 100\n",
    "batch_size = 1\n",
    "\n",
    "train_dataset = CustomDataset(train_pairs, input_vocab, output_vocab)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "encoder = EncoderLSTM(input_vocab.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_vocab.n_words).to(device)\n",
    "#decoder_no_attn = DecoderLSTM(hidden_size, output_vocab.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 100000, print_every=10000, plot_every=5000)\n",
    "\n",
    "torch.save(encoder, \"encoder_LSTM_exp3_wPad_wAttn_8\")\n",
    "torch.save(decoder, \"decoder_LSTM_exp3_wPad_wAttn_8\")\n",
    "\n",
    "eval = [test_pairs[i] for i in range(len(test_pairs))]\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "result_8 = evaluate(encoder, decoder, eval, input_vocab, output_vocab)\n",
    "results.append(result_8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP FOR EACH EXPERIMENT!\n",
    "#16\n",
    "def prepareData():\n",
    "    print(\"reading train lines...\")\n",
    "    lines = open(\"ATNLProject-main/Data/add_prim_split/with_additional_examples/tasks_train_addprim_complex_jump_num16_rep1.txt\").read().strip().split(\"\\n\")\n",
    "    train_pairs = [[re.sub(\"IN: \", '', seq.strip()) for seq in seq.split(\"OUT: \")] for seq in lines]\n",
    "\n",
    "    print(\"reading test lines...\")\n",
    "    lines = open(\"ATNLProject-main/Data/add_prim_split/with_additional_examples/tasks_test_addprim_complex_jump_num16_rep1.txt\").read().strip().split(\"\\n\")\n",
    "    test_pairs = [[re.sub(\"IN: \", '', seq.strip()) for seq in seq.split(\"OUT: \")] for seq in lines]\n",
    "    \n",
    "\n",
    "    print(\"Building dictionaries...\")\n",
    "    input_lang = Vocab('in')\n",
    "    output_lang = Vocab('out')\n",
    "\n",
    "    print(\"Data loading complete\")\n",
    "    \n",
    "    return input_lang, output_lang, train_pairs, test_pairs\n",
    "\n",
    "input_lang, output_lang, train_pairs, test_pairs = prepareData()\n",
    "\n",
    "input_vocab = Vocab('IN')\n",
    "output_vocab = Vocab('OUT')\n",
    "for line in train_pairs:\n",
    "    input_vocab.addSentence(line[0])\n",
    "    output_vocab.addSentence(line[1])\n",
    "\n",
    "\n",
    "hidden_size = 100\n",
    "batch_size = 1\n",
    "\n",
    "train_dataset = CustomDataset(train_pairs, input_vocab, output_vocab)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "encoder = EncoderLSTM(input_vocab.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_vocab.n_words).to(device)\n",
    "#decoder_no_attn = DecoderLSTM(hidden_size, output_vocab.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 100000, print_every=10000, plot_every=5000)\n",
    "\n",
    "torch.save(encoder, \"encoder_LSTM_exp3_wPad_wAttn_16\")\n",
    "torch.save(decoder, \"decoder_LSTM_exp3_wPad_wAttn_16\")\n",
    "\n",
    "eval = [test_pairs[i] for i in range(len(test_pairs))]\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "result_16 = evaluate(encoder, decoder, eval, input_vocab, output_vocab)\n",
    "results.append(result_16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP FOR EACH EXPERIMENT!\n",
    "#32\n",
    "def prepareData():\n",
    "    print(\"reading train lines...\")\n",
    "    lines = open(\"ATNLProject-main/Data/add_prim_split/with_additional_examples/tasks_train_addprim_complex_jump_num32_rep1.txt\").read().strip().split(\"\\n\")\n",
    "    train_pairs = [[re.sub(\"IN: \", '', seq.strip()) for seq in seq.split(\"OUT: \")] for seq in lines]\n",
    "\n",
    "    print(\"reading test lines...\")\n",
    "    lines = open(\"ATNLProject-main/Data/add_prim_split/with_additional_examples/tasks_test_addprim_complex_jump_num32_rep1.txt\").read().strip().split(\"\\n\")\n",
    "    test_pairs = [[re.sub(\"IN: \", '', seq.strip()) for seq in seq.split(\"OUT: \")] for seq in lines]\n",
    "    \n",
    "\n",
    "    print(\"Building dictionaries...\")\n",
    "    input_lang = Vocab('in')\n",
    "    output_lang = Vocab('out')\n",
    "\n",
    "    print(\"Data loading complete\")\n",
    "    \n",
    "    return input_lang, output_lang, train_pairs, test_pairs\n",
    "\n",
    "input_lang, output_lang, train_pairs, test_pairs = prepareData()\n",
    "\n",
    "input_vocab = Vocab('IN')\n",
    "output_vocab = Vocab('OUT')\n",
    "for line in train_pairs:\n",
    "    input_vocab.addSentence(line[0])\n",
    "    output_vocab.addSentence(line[1])\n",
    "\n",
    "\n",
    "hidden_size = 100\n",
    "batch_size = 1\n",
    "\n",
    "train_dataset = CustomDataset(train_pairs, input_vocab, output_vocab)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "encoder = EncoderLSTM(input_vocab.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_vocab.n_words).to(device)\n",
    "#decoder_no_attn = DecoderLSTM(hidden_size, output_vocab.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 100000, print_every=10000, plot_every=5000)\n",
    "\n",
    "torch.save(encoder, \"encoder_LSTM_exp3_wPad_wAttn_32\")\n",
    "torch.save(decoder, \"decoder_LSTM_exp3_wPad_wAttn_32\")\n",
    "\n",
    "eval = [test_pairs[i] for i in range(len(test_pairs))]\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "result_32 = evaluate(encoder, decoder, eval, input_vocab, output_vocab)\n",
    "results.append(result_32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "composed_commands = ['1', '2', '4', '8', '16', '32']\n",
    "accuracy_values = results[1:]\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(composed_commands, accuracy_values, color='lightsteelblue')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Number of Composed Commands Used for Training')\n",
    "plt.ylabel('Accuracy on New Commands (%)')\n",
    "plt.title('Zero-Shot Generalization After Adding \"Jump\" Commands')\n",
    "plt.ylim(0, 100)  # Setting the limit for y-axis to 100% for clarity\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
